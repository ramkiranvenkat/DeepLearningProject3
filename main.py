# -*- coding: utf-8 -*-
"""main3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14PPZ3RTWtIu3m6xBDEZbwftzQn_DsO6t
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import urllib
import sys
import os
import zipfile

glovesize =200
concatsize = 20
glove_vectors_file = "./Data/glove.6B." + str(glovesize) + "d.txt"
snli_dev_file = "./Data/snli_1.0/snli_1.0_dev.txt"
snli_test_file = "./Data/snli_1.0/snli_1.0_test.txt"
snli_train_file = "./Data/snli_1.0/snli_1.0_train.txt"

zip_ref = zipfile.ZipFile("./Data/snli_1.0.zip", 'r')
# https://drive.google.com/open?id=1SRiw7N2kl45qNJUWCYmzdIhaIP-udYbn
zip_ref.extractall("./Data")
zip_ref.close()

zip_ref = zipfile.ZipFile("./Data/glove.6B.zip", 'r')
# https://drive.google.com/open?id=1ixGaHUyHRhCQ60rSvwNzMPJc7aWAWtOt
zip_ref.extractall("./Data")
zip_ref.close()

glove_wordmap = {}
with open(glove_vectors_file, "r") as glove:
    for line in glove:
        name, vector = tuple(line.split(" ", 1))
        glove_wordmap[name] = np.fromstring(vector, sep=" ")

import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
def pre_proc(doc):
    wt = word_tokenize(doc)
    punc = [re.sub(r'[^\w\s]', '', w) for w in wt if re.sub(r'[^\w\s]', '', w) != '']
    lemm = [lemmatizer.lemmatize(w, pos='v') for w in punc]
    filtered_sentence = [w for w in lemm if not w in stop_words]
    return filtered_sentence   
def pre_proc_tfidf(doc):
    wt = word_tokenize(doc)
    punc = [re.sub(r'[^\w\s]', '', w) for w in wt if re.sub(r'[^\w\s]', '', w) != '']
    lemm = [lemmatizer.lemmatize(w, pos='v') for w in punc]
    filtered_sentence = [w for w in lemm if not w in stop_words]
    sentenceOut = ''
    for word in filtered_sentence:
        sentenceOut += word+" "
    #print(doc+'-->'+sentenceOut)
    sentenceOut = sentenceOut.strip(" ")
    return sentenceOut, filtered_sentence

def convert2input(s1,s2,gl,n):
    sntns1 = []
    sntns2 = []
    label = []
    for i in range(n):
        wl1 = pre_proc(s1[i].lower())
        wl2 = pre_proc(s2[i].lower())
        #if (i%10000 == 0):
        #print(i)
        if (len(wl1)>0 and len(wl1)>0):
            if (gl[i] == 'contradiction'):
                label.append(np.array([1,0,0,0]))
            elif (gl[i] == 'entailment'):
                label.append(np.array([0,1,0,0]))
            elif (gl[i] == 'neutral'):
                label.append(np.array([0,0,1,0]))
            else:
                label.append(np.array([0,0,0,1]))
            
            gac1 = np.zeros((concatsize,glovesize))
            gac2 = np.zeros((concatsize,glovesize))
            igac = 0
            wc = 0
            for word in wl1:
                if word in glove_wordmap and wc < concatsize:
                    gac1[igac,:] = glove_wordmap[word][0:glovesize]
                    igac+=1
                    wc+=1
            igac = 0
            wc = 0
            for word in wl2:
                if word in glove_wordmap and wc < concatsize:
                    gac2[igac,:] = glove_wordmap[word][0:glovesize]
                    igac+=1
                    wc+=1
            gac1 = np.array(gac1)
            gac1 = np.array(gac1)
            sntns1.append(gac1)
            sntns2.append(gac2)
    return  np.array(sntns1), np.array(sntns2), np.array(label)

import pandas as pd

def extractData(filename,smplSz):
    data = pd.read_csv(filename,sep='\t')
    data.dropna(inplace=True, subset=['sentence2']) # Removes the rows where sentence2 is empty
    blanks =[]
    for i,x,y in data[['sentence1','sentence2']].itertuples():
        if (x.isspace() or y.isspace()):
            blanks.append(i)

    if (len(blanks) != 0):
        data.drop(blanks, inplace=True)
    if (smplSz > len(data['gold_label'].tolist())):
        smplSz = len(data['gold_label'].tolist())

    s1, s2, l = convert2input(data['sentence1'].tolist(),data['sentence2'].tolist(),data['gold_label'].tolist(),smplSz)
    return s1, s2, l

def tfidfData(filename):
    data = pd.read_csv(filename,sep='\t')
    data.dropna(inplace=True, subset=['sentence2']) # Removes the rows where sentence2 is empty
    blanks =[]
    for i,x,y in data[['sentence1','sentence2']].itertuples():
        if (x.isspace() or y.isspace()):
            blanks.append(i)

    if (len(blanks) != 0):
        data.drop(blanks, inplace=True)
    return data['sentence1'].tolist(),data['sentence2'].tolist(),data['gold_label'].tolist()

s1, s2, gl = tfidfData(snli_train_file)
s1T, s2T, glT = tfidfData(snli_test_file)

gl = pd.Series(gl)
s1 = pd.Series(s1)
s2 = pd.Series(s2)
glT = pd.Series(glT)
s1T = pd.Series(s1T)
s2T = pd.Series(s2T)

corpus = []
sntns1 = []
sntns2 = []
vocab =set()
label = []
for i in range(len(gl)):
    so1, wl1 = pre_proc_tfidf(s1[i].lower())
    so2, wl2 = pre_proc_tfidf(s2[i].lower())
    #print(so1,"---",so2,'\n',so1.split(),'---',so2.split())
    if (len(so1)>0 and len(so2)>0):
        corpus.append(so1)
        corpus.append(so2)
        sntns1.append(so1)
        sntns2.append(so2)
        vocab.update(wl1)
        vocab.update(wl2)
        label.append(gl[i])
vocab = list(vocab)

corpusT = []
sntns1T = []
sntns2T = []
labelT = []
for i in range(len(glT)):
    so1T, wl1T = pre_proc_tfidf(s1T[i].lower())
    so2T, wl2T = pre_proc_tfidf(s2T[i].lower())
    #print(so1,"---",so2,'\n',so1.split(),'---',so2.split())
    if (len(so1T)>0 and len(so2T)>0):
        corpusT.append(so1T)
        corpusT.append(so2T)
        sntns1T.append(so1T)
        sntns2T.append(so2T)
        labelT.append(glT[i])

sntns1T = pd.Series(sntns1T)
sntns2T = pd.Series(sntns2T)
labelT = pd.Series(labelT)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

cv = CountVectorizer(input='content',
    encoding='utf-8',
    decode_error='strict',
    strip_accents=None,
    lowercase=True,
    preprocessor=None,
    tokenizer=None,
    stop_words=None,
    token_pattern='(?u)\\b\\w\\w+\\b',
    ngram_range=(1, 1),
    analyzer='word',
    max_df=1.0,
    min_df=1,
    max_features=None,
    vocabulary=None,
    binary=False)

cv.fit(pd.Series(corpus))

test_sent1_vect = cv.transform(sntns1T)
test_sent2_vect = cv.transform(sntns2T)
sent1_vect = cv.transform(sntns1)
sent2_vect = cv.transform(sntns2)

tfidf_transformer = TfidfTransformer()
sent1_tfidf = tfidf_transformer.fit_transform(sent1_vect)
sent2_tfidf = tfidf_transformer.fit_transform(sent2_vect)
test_sent1_tfidf = tfidf_transformer.transform(test_sent1_vect)
test_sent2_tfidf = tfidf_transformer.transform(test_sent2_vect)

import scipy
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from scipy.sparse import hstack


X_test = hstack([test_sent1_tfidf, test_sent2_tfidf])
X_test = preprocessing.scale(X_test, with_mean=False)

import pickle
pkl_filename = "./models/lr_model.pkl"
#for link https://drive.google.com/open?id=1U07Rfi7JwtWswoADX-obbtIrTjogO4UQ
with open(pkl_filename, 'rb') as file:
    pickle_model = pickle.load(file)
    
predictions = pickle_model.predict(X_test)

#print(predictions)
o2 = open('tfidf.txt','a')
for i in range(predictions.shape[0]):
    o2.write(predictions[i]+'\n')
o2.close()

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report

print("TFIDF-LOGISTIC REGRESSION ACC: ",metrics.accuracy_score(labelT, predictions))
#print(confusion_matrix(labelT, predictions))
#print(classification_report(labelT, predictions))

stest1, stest2, ltest = extractData(snli_test_file,10000)
cmodel = tf.keras.models.load_model('./models/PNL-Mask-3LSTM-g200-3_256-128-do-128-do-4-b512.h5')
# for link"https://drive.google.com/open?id=1otgaBxjSWYSk7q7VajCYlvdwd88IBVgw"
a = cmodel.predict([stest1,stest2],32)
oitr = np.argmax(a,axis=1)
iitr = np.argmax(ltest,axis=1)

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report

print("DEEP LEARNING ACC: ",metrics.accuracy_score(iitr, oitr))
#print(confusion_matrix(iitr, oitr))
#print(classification_report(iitr, oitr))

o2 = open('deep_model.txt','a')
lcT = []
for i in range(oitr.shape[0]):
    if (oitr[i] == 0):
        o2.write("contradiction"+'\n')
        lcT.append(np.array([1,0,0,0]))
        continue
    if (oitr[i] == 1):
        o2.write("entailment"+'\n')
        lcT.append(np.array([0,1,0,0]))
        continue
    if (oitr[i] == 2):
        o2.write("neutral"+'\n')
        lcT.append(np.array([0,0,1,0]))
        continue
    o2.write("-"+'\n')
    lcT.append(np.array([0,0,0,1]))
o2.close()
lcT = np.array(lcT)
