# -*- coding: utf-8 -*-
"""glove.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1krSxQs0aORuK16XYSoqPh7ZYSgvERpzs
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import urllib
import sys
import os
import zipfile

glovesize =200
glove_vectors_file = "/tmp/glove.6B." + str(glovesize) + "d.txt"
snli_dev_file = "/tmp/snli_1.0/snli_1.0_dev.txt"
snli_test_file = "/tmp/snli_1.0/snli_1.0_test.txt"
snli_train_file = "/tmp/snli_1.0/snli_1.0_train.txt"

from google.colab import drive
drive.mount('/content/drive')
zip_ref = zipfile.ZipFile("/content/drive/My Drive/snli_1.0.zip", 'r')
zip_ref.extractall("/tmp")
zip_ref.close()

zip_ref = zipfile.ZipFile("/content/drive/My Drive/glove.6B.zip", 'r')
zip_ref.extractall("/tmp")
zip_ref.close()

"""
from six.moves.urllib.request import urlretrieve
   
if (not os.path.isfile(glove_zip_file) and not os.path.isfile(glove_vectors_file)):
  urlretrieve ("https://drive.google.com/open?id=1ixGaHUyHRhCQ60rSvwNzMPJc7aWAWtOt", glove_zip_file)

if (not os.path.isfile(snli_zip_file) and not os.path.isfile(snli_dev_file)):
  urlretrieve ("https://drive.google.com/open?id=1SRiw7N2kl45qNJUWCYmzdIhaIP-udYbn", snli_zip_file)

from six.moves.urllib.request import urlretrieve
    
if (not os.path.isfile(glove_zip_file) and not os.path.isfile(glove_vectors_file)):
  urlretrieve ("http://nlp.stanford.edu/data/glove.6B.zip", glove_zip_file)

if (not os.path.isfile(snli_zip_file) and not os.path.isfile(snli_dev_file)):
  urlretrieve ("https://nlp.stanford.edu/projects/snli/snli_1.0.zip", snli_zip_file)
"""

"""
def unzip_single_file(zip_file_name, output_file_name):
    
    #If the outFile is already created, don't recreate
    #If the outFile does not exist, create it from the zipFile
    if not os.path.isfile(output_file_name):
        with open(output_file_name, 'wb') as out_file:
            with zipfile.ZipFile(zip_file_name) as zipped:
                for info in zipped.infolist():
                    if output_file_name in info.filename:
                        with zipped.open(info) as requested_file:
                            out_file.write(requested_file.read())
                            return

unzip_single_file(glove_zip_file, glove_vectors_file)
unzip_single_file(snli_zip_file, snli_dev_file)
unzip_single_file(snli_zip_file, snli_test_file)
unzip_single_file(snli_zip_file, snli_train_file)
"""

glove_wordmap = {}
with open(glove_vectors_file, "r") as glove:
    for line in glove:
        name, vector = tuple(line.split(" ", 1))
        glove_wordmap[name] = np.fromstring(vector, sep=" ")

import pandas as pd
data =  pd.read_csv(snli_train_file,sep='\t')
dataT =  pd.read_csv(snli_test_file,sep='\t')
dataV =  pd.read_csv(snli_dev_file,sep='\t')
data.dropna(inplace=True, subset=['sentence2']) # Removes the rows where sentence2 is empty
dataT.dropna(inplace=True, subset=['sentence2']) # Removes the rows where sentence2 is empty
dataV.dropna(inplace=True, subset=['sentence2']) # Removes the rows where sentence2 is empty

blanks =[]
for i,x,y in data[['sentence1','sentence2']].itertuples():
    if (x.isspace() or y.isspace()):
        blanks.append(i)

if (len(blanks) != 0):
    data.drop(blanks, inplace=True)

blanksT =[]
for i,x,y in dataT[['sentence1','sentence2']].itertuples():
    if (x.isspace() or y.isspace()):
        blanksT.append(i)

if (len(blanksT) != 0):
    dataT.drop(blanksT, inplace=True)

blanksV =[]
for i,x,y in dataV[['sentence1','sentence2']].itertuples():
    if (x.isspace() or y.isspace()):
        blanksV.append(i)

if (len(blanksV) != 0):
    dataV.drop(blanksV, inplace=True)
    

gl = data['gold_label'].tolist()
s1 = data['sentence1'].tolist()
s2 = data['sentence2'].tolist()

glT = dataT['gold_label'].tolist()
s1T = dataT['sentence1'].tolist()
s2T = dataT['sentence2'].tolist()

glV = dataV['gold_label'].tolist()
s1V = dataV['sentence1'].tolist()
s2V = dataV['sentence2'].tolist()

import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
def pre_proc(doc):
    wt = word_tokenize(doc)
    punc = [re.sub(r'[^\w\s]', '', w) for w in wt if re.sub(r'[^\w\s]', '', w) != '']
    lemm = [lemmatizer.lemmatize(w, pos='v') for w in punc]
    filtered_sentence = [w for w in lemm if not w in stop_words]
    return filtered_sentence

sntnsc = []
label = []
#maxsize = 0
concatsize = 40
for i in range(0*int(len(gl)) + 1*10000):
    wl1 = pre_proc(s1[i].lower())
    wl2 = pre_proc(s2[i].lower())
    if (i%10000 == 0):
        print(i)
    if (len(wl1)>0 and len(wl1)>0):
        if (gl[i] == 'contradiction'):
            label.append(np.array([1,0,0,0]))
        elif (gl[i] == 'entailment'):
            label.append(np.array([0,1,0,0]))
        elif (gl[i] == 'neutral'):
            label.append(np.array([0,0,1,0]))
        else:
            label.append(np.array([0,0,0,1]))
        
        gac = np.zeros((concatsize,glovesize))
        
        igac = 0
        wc = 0
        for word in wl1:
            if word in glove_wordmap and wc < concatsize/2:
                gac[igac,:] = glove_wordmap[word][0:glovesize]
                igac+=1
                wc+=1
        wc = 0
        for word in wl2:
            if word in glove_wordmap and wc < concatsize/2:
                gac[igac,:] = glove_wordmap[word][0:glovesize]
                igac+=1
                wc+=1
        gac = np.array(gac)
        sntnsc.append(gac)
        #if (gac.shape[0] > maxsize):
            #maxsize = gac.shape[0]

#sntnsc = tf.convert_to_tensor(sntnsc)
#label = tf.convert_to_tensor(label)
sntnsc = np.array(sntnsc)
label = np.array(label)
print(sntnsc.shape,label.shape)

print(sntnsc[:,10,:])

sntnscT = []
labelT = []
for i in range(len(glT)):
    wl1 = pre_proc(s1T[i].lower())
    wl2 = pre_proc(s2T[i].lower())
    if (len(wl1)>0 and len(wl1)>0):
        if (glT[i] == 'contradiction'):
            labelT.append(np.array([1,0,0,0]))
        elif (glT[i] == 'entailment'):
            labelT.append(np.array([0,1,0,0]))
        elif (glT[i] == 'neutral'):
            labelT.append(np.array([0,0,1,0]))
        else:
            labelT.append(np.array([0,0,0,1]))
      
        gac = np.zeros((concatsize,glovesize))

        igac = 0
        wc = 0
        for word in wl1:
            if word in glove_wordmap and wc < concatsize/2:
                gac[igac,:] = glove_wordmap[word][0:glovesize]
                igac+=1
                wc+=1
        
        wc = 0
        for word in wl2:
            if word in glove_wordmap and wc < concatsize/2:
                gac[igac,:] = glove_wordmap[word][0:glovesize]
                igac+=1
                wc+=1
        gac = np.array(gac)
        sntnscT.append(gac)

#sntnscT = tf.convert_to_tensor(sntnscT)
#labelT = tf.convert_to_tensor(labelT)
sntnscT = np.array(sntnscT)
labelT = np.array(labelT)
print(labelT.shape,sntnscT.shape)

sntnscV = []
labelV = []
for i in range(len(glV)):
    wl1 = pre_proc(s1V[i].lower())
    wl2 = pre_proc(s2V[i].lower())
    if (len(wl1)>0 and len(wl1)>0):
        if (glV[i] == 'contradiction'):
            labelV.append(np.array([1,0,0,0]))
        elif (glV[i] == 'entailment'):
            labelV.append(np.array([0,1,0,0]))
        elif (glV[i] == 'neutral'):
            labelV.append(np.array([0,0,1,0]))
        else:
            labelV.append(np.array([0,0,0,1]))
      
        gac = np.zeros((concatsize,glovesize))

        igac = 0
        wc = 0
        for word in wl1:
            if word in glove_wordmap and wc < concatsize/2:
                gac[igac,:] = glove_wordmap[word][0:glovesize]
                igac+=1
                wc+=1
        
        wc = 0
        for word in wl2:
            if word in glove_wordmap and wc < concatsize/2:
                gac[igac,:] = glove_wordmap[word][0:glovesize]
                igac+=1
                wc+=1
        gac = np.array(gac)
        sntnscV.append(gac)

sntnscV = np.array(sntnscV)
labelV = np.array(labelV)
print(labelV.shape,sntnscV.shape)

#Training
class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy')>0.90):
            print("\nReached 90% accuracy so cancelling training!")
            self.model.stop_training = True

model = tf.keras.models.Sequential([
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=200,input_shape=(40,glovesize),return_sequences=False)),

    #tf.keras.layers.Dropout(0.5),
    #tf.keras.layers.Dense(64, activation="relu"),
    
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(4, activation="sigmoid")
])

callbacks = myCallback()
initial_lr = 0.001
def scheduler(epoch):
    if epoch < 25:
        return initial_lr
    else:
        return initial_lr * tf.math.exp(0.1 * (20 - epoch))

lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)
earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, restore_best_weights=True)

model.compile(loss="categorical_crossentropy",
              optimizer="adam",
              metrics=["accuracy"])

print(tf.keras.backend.eval(model.optimizer.lr))
batch_size=128
with tf.device('/device:GPU:0'):
  history = model.fit(sntnsc,label,validation_data=(sntnscV, labelV),
                      epochs=20,batch_size=batch_size,callbacks=[callbacks, earlystop],steps_per_epoch=(sntnsc.shape[0]//batch_size))

model.save('LSTM-RNN-glove-g'+str(glovesize)+'-200-32-4-b128.h5')

model.evaluate(sntnscT,labelT,32)

import matplotlib.image  as mpimg
import matplotlib.pyplot as plt

acc=history.history['accuracy']
loss=history.history['loss']
vacc=history.history['val_accuracy']
vloss=history.history['val_loss']

epochs=range(len(loss)) 

plt.plot(epochs, acc, 'r')
plt.plot(epochs, loss, 'b')
plt.plot(epochs, vacc, 'g')
plt.plot(epochs, vloss, 'k')
plt.title('Accuracy and Loss')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["Acc", "Loss","V Acc","V loass"])

plt.figure()