# -*- coding: utf-8 -*-
"""mask.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MUsRuDhZbwHSJ8ILesDtxVOcajjrzAOO
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import urllib
import sys
import os
import zipfile

glovesize =200
concatsize = 20
glove_vectors_file = "/home/glove.6B." + str(glovesize) + "d.txt"
snli_dev_file = "/home/snli_1.0/snli_1.0_dev.txt"
snli_test_file = "/home/snli_1.0/snli_1.0_test.txt"
snli_train_file = "/home/snli_1.0/snli_1.0_train.txt"

print(glove_vectors_file)

from google.colab import drive
drive.mount('/content/drive')
zip_ref = zipfile.ZipFile("/content/drive/My Drive/snli_1.0.zip", 'r')
zip_ref.extractall("/home")
zip_ref.close()

zip_ref = zipfile.ZipFile("/content/drive/My Drive/glove.6B.zip", 'r')
zip_ref.extractall("/home")
zip_ref.close()

glove_wordmap = {}
with open(glove_vectors_file, "r") as glove:
    for line in glove:
        name, vector = tuple(line.split(" ", 1))
        glove_wordmap[name] = np.fromstring(vector, sep=" ")

import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
def pre_proc(doc):
    wt = word_tokenize(doc)
    punc = [re.sub(r'[^\w\s]', '', w) for w in wt if re.sub(r'[^\w\s]', '', w) != '']
    lemm = [lemmatizer.lemmatize(w, pos='v') for w in punc]
    filtered_sentence = [w for w in lemm if not w in stop_words]
    return filtered_sentence

def convert2input(s1,s2,gl,n):
    sntns1 = []
    sntns2 = []
    label = []
    for i in range(n):
        wl1 = pre_proc(s1[i].lower())
        wl2 = pre_proc(s2[i].lower())
        if (i%10000 == 0):
            print(i)
        if (len(wl1)>0 and len(wl1)>0):
            if (gl[i] == 'contradiction'):
                label.append(np.array([1,0,0,0]))
            elif (gl[i] == 'entailment'):
                label.append(np.array([0,1,0,0]))
            elif (gl[i] == 'neutral'):
                label.append(np.array([0,0,1,0]))
            else:
                label.append(np.array([0,0,0,1]))
            
            gac1 = np.zeros((concatsize,glovesize))
            gac2 = np.zeros((concatsize,glovesize))
            igac = 0
            wc = 0
            for word in wl1:
                if word in glove_wordmap and wc < concatsize:
                    gac1[igac,:] = glove_wordmap[word][0:glovesize]
                    igac+=1
                    wc+=1
            igac = 0
            wc = 0
            for word in wl2:
                if word in glove_wordmap and wc < concatsize:
                    gac2[igac,:] = glove_wordmap[word][0:glovesize]
                    igac+=1
                    wc+=1
            gac1 = np.array(gac1)
            gac1 = np.array(gac1)
            sntns1.append(gac1)
            sntns2.append(gac2)
    return  np.array(sntns1), np.array(sntns2), np.array(label)

import pandas as pd

def extractData(filename,smplSz):
    data = pd.read_csv(filename,sep='\t')
    data.dropna(inplace=True, subset=['sentence2']) # Removes the rows where sentence2 is empty
    blanks =[]
    for i,x,y in data[['sentence1','sentence2']].itertuples():
        if (x.isspace() or y.isspace()):
            blanks.append(i)

    if (len(blanks) != 0):
        data.drop(blanks, inplace=True)
    if (smplSz > len(data['gold_label'].tolist())):
        smplSz = len(data['gold_label'].tolist())

    s1, s2, l = convert2input(data['sentence1'].tolist(),data['sentence2'].tolist(),data['gold_label'].tolist(),smplSz)
    return s1, s2, l

strain1, strain2, ltrain = extractData(snli_train_file,150000)

print(strain1.shape,strain2.shape,ltrain.shape)

stest1, stest2, ltest = extractData(snli_test_file,10000)

print(stest1.shape,stest2.shape,ltest.shape)

sval1, sval2, lval = extractData(snli_dev_file,10000)

print(sval1.shape,sval2.shape,lval.shape)

#Training
class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy')>0.90):
            print("\nReached 90% accuracy so cancelling training!")
            self.model.stop_training = True


inputs1 = tf.keras.layers.Input(shape=(concatsize,glovesize))
inputs2 = tf.keras.layers.Input(shape=(concatsize,glovesize))

maskLayer = tf.keras.layers.Masking(mask_value=0.,input_shape=(concatsize, glovesize))
mask1 = maskLayer(inputs1)
mask2 = maskLayer(inputs2)

LI_LSTM1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=256,return_sequences=True,dropout=0.2, recurrent_dropout=0.2))
LI_LSTM2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=256,return_sequences=True,dropout=0.2, recurrent_dropout=0.2))
LI_LSTM3 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=256,return_sequences=False,dropout=0.2, recurrent_dropout=0.2))

seq1o1 = LI_LSTM1(mask1)
seq2o1 = LI_LSTM1(mask2)

seq1o2 = LI_LSTM2(seq1o1)
seq2o2 = LI_LSTM2(seq2o1)

seq1o = LI_LSTM3(seq1o2)
seq2o = LI_LSTM3(seq2o2)

#seq1o = tf.reduce_sum(seq1o3,axis=1)
#seq2o = tf.reduce_sum(seq2o3,axis=1)

DO1 = tf.keras.layers.Dropout(0.5)
DO2 = tf.keras.layers.Dropout(0.5)
#d1o = DO1(seq1o)
#d2o = DO1(seq2o)
#tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=64,input_shape=(40,50),return_sequences=False,activation="sigmoid"))
#tf.keras.layers.Dropout(0.5)
#tf.keras.layers.LSTM(units=64,input_shape=(40,50),return_sequences=False,activation="sigmoid")
dense_input2 = tf.keras.layers.concatenate([seq1o,seq2o],axis=1)

LI_DENSE2 = tf.keras.layers.Dense(128, activation="relu")
dense_input1 = LI_DENSE2(dense_input2)
dense_input1_do = DO1(dense_input1)

LI_DENSE1 = tf.keras.layers.Dense(128, activation="relu")
dense_input = LI_DENSE1(dense_input1_do)
dense_input_do = DO1(dense_input)

LI_DENSE  = tf.keras.layers.Dense(4, activation="sigmoid")
op = LI_DENSE(dense_input_do)

model = tf.keras.models.Model(inputs=[inputs1, inputs2], outputs=op)

model.summary()

callbacks = myCallback()
initial_lr = 0.001
def scheduler(epoch):
    if epoch < 25:
        return initial_lr
    else:
        return initial_lr * tf.math.exp(0.1 * (20 - epoch))

lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)
earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, restore_best_weights=True)

#model.summary()
model.compile(loss="categorical_crossentropy",
              optimizer="adam",
              metrics=["accuracy"])

print(tf.keras.backend.eval(model.optimizer.lr))
batch_size=512
with tf.device('/device:GPU:0'):
  history = model.fit([strain1,strain2],ltrain,validation_data=([sval1,sval2], lval),
                      epochs=50,batch_size=batch_size,callbacks=[callbacks,earlystop],steps_per_epoch=(strain1.shape[0]//batch_size))

model.save('PNL-Mask-3LSTM-g'+str(glovesize)+'-3*256-128-do-128-do-4(return_sequences=False)-b512.h5')

model.evaluate([stest1,stest2],ltest,32)

import matplotlib.image  as mpimg
import matplotlib.pyplot as plt

acc=history.history['accuracy']
loss=history.history['loss']
vacc=history.history['val_accuracy']
vloss=history.history['val_loss']

epochs=range(len(loss)) 

plt.plot(epochs, acc, 'r')
plt.plot(epochs, loss, 'b')
plt.plot(epochs, vacc, 'g')
plt.plot(epochs, vloss, 'k')
plt.title('Accuracy and Loss')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["Acc", "Loss","V Acc","V loass"])

plt.figure()